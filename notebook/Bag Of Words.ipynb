{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    _bag_of_words = [collections.Counter(re.findall(r'\\w+',word)) for word in text]\n",
    "    bag_of_words = sum(_bag_of_words, collections.Counter())\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=['In order to perform machine learning on text documents, the raw (text)',\n",
    " 'data cannot be fed directly to algorithm as these algorithms expect numerical feature vectors',\n",
    " 'so instead we need to turn the text content into numerical feature vectors.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'to': 3, 'text': 3, 'the': 2, 'numerical': 2, 'feature': 2, 'vectors': 2, 'In': 1, 'order': 1, 'perform': 1, 'machine': 1, 'learning': 1, 'on': 1, 'documents': 1, 'raw': 1, 'data': 1, 'cannot': 1, 'be': 1, 'fed': 1, 'directly': 1, 'algorithm': 1, 'as': 1, 'these': 1, 'algorithms': 1, 'expect': 1, 'so': 1, 'instead': 1, 'we': 1, 'need': 1, 'turn': 1, 'content': 1, 'into': 1})\n"
     ]
    }
   ],
   "source": [
    "sample_word_tokens_bow = bag_of_words(text=a)\n",
    "print(sample_word_tokens_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Extracting features  or feature encoding from text files\n",
    "\n",
    "In order to perform machine learning on text documents, the raw (text) data cannot be fed directly to algorithm as these algorithms expect numerical feature vectors so instead we need to turn the text content into numerical feature vectors.\n",
    "\n",
    "From the [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html):\n",
    "<b>\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors.\n",
    "</b>\n",
    "\n",
    "In creating a classifier it is important to decide what features of the input are relevant, and how to encode those features. When we consider a textual data such as a sentence or a document  for instance the observable features are the counts and the order of the letters and the words within the text so we need a way to extract these  features. There are several ways of extracting features from a textual data but in this tutorial we will consider a very common feature extraction procedures for sentences and documents known as the <b>\n",
    "bag-of-words approach (BOW)</b> which looks at the histogram of the words within the text ( considering each word count as a feature.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words (BOWs) \n",
    "Is a feature extraction technique used for extracting features from textual data for modeling machine learning algorithm and is commonly used in problems such as language modeling and document classification.  A bag-of-words is a representation of textual data, describing the occurrence of words within a sentence or document, disregarding grammar and the order of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>How does Bag of Words Works</b></p>\n",
    "In order to understand how bag of words works let assume we have two simple text documents:\n",
    "```md\n",
    "1. Boys like playing football and Emma is a boy so Emma likes playing football\n",
    "\n",
    "2  Mary likes watching movies \n",
    "\n",
    "```\n",
    "\n",
    "Based on these two text documents, a list of token (words) for each document is as follows\n",
    "\n",
    "```javascript\n",
    "'Boys', 'like', 'playing', 'football', 'and', 'Emma', 'is', 'a', 'boy', 'so', 'Emma', 'likes', 'playing', 'football'\n",
    "\n",
    "\n",
    "'Mary', 'likes', 'watching', 'movies'\n",
    "```\n",
    "\n",
    "\n",
    "denoting document1 by doc1 and 2  by doc2, we will construct a dictionary (key->value pair) of\n",
    "words for both doc1 and doc2 where each key is the word, and each value is the number of occurrences of that word in the given text document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```javascript\n",
    "doc1={ 'a' : 1, 'and' : 1, 'boy' : 1, 'Boys' : 1, 'Emma' : 2, 'football' : 2, 'is' : 1,  'like' : 1,  'likes' : 1, 'playing' : 2,   'so' : 1}\n",
    "\n",
    "dco2={'likes' : 1, 'Mary' : 1,  'movies' : 1 ,'watching' : 1}\n",
    "```\n",
    "\n",
    "<b>NOTE :</b> the order of the words is not important\n",
    "\n",
    "\n",
    "Putting everything together and considering **a** as a stop word, features extracted using bag of words for these documents will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vocabulary of words\n",
    "and, boy, boys, emma,  football, is, like, likes, mary, movies, playing, so, watching\n",
    "\n",
    "\n",
    "\n",
    "<style>\n",
    "table, th, td {\n",
    "    border: 1px solid black;\n",
    "    border-collapse: collapse;\n",
    "}\n",
    "th, td {\n",
    "    padding: 15px;\n",
    "}\n",
    "</style>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th><th>and</th><th>boy</th><th>boys</th><th>emma</th><th>football</th>\n",
    "        <th>is</th> <th>like</th><th>likes</th><th>mary</th><th>movies</th>\n",
    "        <th>playing</th><th>so</th>\n",
    "        <th>watching</th>\n",
    "    </tr>\n",
    "\n",
    "<tr>\n",
    "    <td>Doc1</td><td>1</td><td>1</td><td>1</td><td>2</td><td>2</td><td>1</td>\n",
    "    <td>1</td><td>1</td><td>0</td><td>0</td><td>2</td><td>1</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Doc2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td>\n",
    "    <td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=['Boys like playing football and Emma is a boy so Emma likes playing football',\n",
    "   \"Mary likes watching movies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Boys', 'like', 'playing', 'football', 'and', 'Emma', 'is', 'a', 'boy', 'so', 'Emma', 'likes', 'playing', 'football']\n",
      "['Mary', 'likes', 'watching', 'movies']\n"
     ]
    }
   ],
   "source": [
    "for i in c:\n",
    "    print(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extr=CountVectorizer()\n",
    "model=feature_extr.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>boy</th>\n",
       "      <th>boys</th>\n",
       "      <th>emma</th>\n",
       "      <th>football</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>likes</th>\n",
       "      <th>mary</th>\n",
       "      <th>movies</th>\n",
       "      <th>playing</th>\n",
       "      <th>so</th>\n",
       "      <th>watching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     and boy boys emma football is like likes mary movies playing so watching\n",
       "doc1   1   1    1    2        2  1    1     1    0      0       2  1        0\n",
       "doc2   0   0    0    0        0  0    0     1    1      1       0  0        1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.toarray(),columns=[feature_extr.get_feature_names()],index=['doc1','doc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Disadvantages</b></p>\n",
    "Although BOWs is very simple to understand and implement, it has some disadvantages which include\n",
    "\n",
    "- highly sparse vectors or matrix as the are  very few non-zero elements in dimensions corresponding to words that occur in the sentence.\n",
    "\n",
    "- Bag of words representation leads to a high dimensional feature vector as the total dimension is the vocabulary size.\n",
    "- Bag of words representation does not consider the semantic relation between words by assuming that the words are independent of each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buiding a Classifier with the features extracted using BOWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset called “Twenty Newsgroups”. which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. <a href='http://qwone.com/~jason/20Newsgroups/'>Official description of theTwenty Newsgroups data</a> will be used as our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will work on a partial dataset with only 11 categories out\n",
    "of the 20 available in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med','sci.electronics',\n",
    "              'sci.space','talk.politics.guns','talk.politics.mideast','talk.politics.misc',\n",
    "              'talk.religion.misc','misc.forsale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_news_train=fetch_20newsgroups(subset='train',categories=categories,\n",
    "                                remove=('footers','headers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'misc.forsale',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_news_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "That's a revisionist account of what happened.  Gritz was well-aware\n",
      "of Duke's presence on the ticket.  Given that Gritz is not at all shy\n",
      "about associating and promoting other white supremacists (such as the\n",
      "Christian Identity movement or Willis Carto)\n",
      " whatever reasons Gritz\n",
      "had to leave the ticket had nothing to do with Duke's presence.\n",
      "\n",
      "\n",
      "I believe Chip Berlet has a Populist Party newsletter from the time with\n",
      "a photo of Gritz happily shaking hands with Duke.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_news_train.data[2].split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_news_train.target[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4==>sci.med\n",
      "0==>alt.atheism\n",
      "7==>talk.politics.guns\n",
      "1==>comp.graphics\n"
     ]
    }
   ],
   "source": [
    "for i in twenty_news_train.target[0:4]:\n",
    "    print(\"{}==>{}\".format(i,twenty_news_train.target_names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_clf=Pipeline([('bows',CountVectorizer()),\n",
    "                   ('sgd',SGDClassifier(max_iter=10,class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bows', CountVectorizer()),\n",
       "                ('sgd', SGDClassifier(class_weight='balanced', max_iter=10))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_clf.fit(twenty_news_train.data,twenty_news_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING NEW INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God is love=>talk.religion.misc\n",
      "OpenGL on the GPU is fast=>comp.graphics\n",
      "I am selling my car=>misc.forsale\n",
      "Nvidia=>misc.forsale\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast','I am selling my car','Nvidia']\n",
    "predict=news_clf.predict(docs_new)\n",
    "for doc, pred in zip(docs_new,predict):\n",
    "    print('{}=>{}'.format(doc,twenty_news_train.target_names[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_news_test=fetch_20newsgroups(subset='test',categories=categories,\n",
    "                                remove=('footers','headers','quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6128140703517588"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction=news_clf.predict(twenty_news_test.data)\n",
    "np.mean(twenty_news_test.target==test_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
